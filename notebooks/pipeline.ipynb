{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# imports for data processing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_predict, KFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# imports for models\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ajouter le chemin du projet au PATH\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from modules.data_import import loadAndMergeData, exportDataCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "data = pd.read_csv(\"../dataClean.csv\")\n",
    "\n",
    "data_num = data.select_dtypes(include=[np.number]) \n",
    "num_attribs = list(data_num)\n",
    "\n",
    "data_cat = data.select_dtypes(include=[object])\n",
    "cat_attribs = list(data_cat)\n",
    "\n",
    "# Pipeline pour les attributs numériques\n",
    "num_pipeline = Pipeline([\n",
    "        #('attribs_adder', CombinedAttributesAdder()), # combination des valeurs pas encore fait\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# Pipeline pours les attributs catégoriel avec un ordinal encoder\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OrdinalEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "data_prepared = full_pipeline.fit_transform(data)\n",
    "\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.get_feature_names_out(cat_attribs))\n",
    "\n",
    "columns = num_attribs + cat_one_hot_attribs\n",
    "\n",
    "data_prepared_df = pd.DataFrame(data_prepared, columns=columns)\n",
    "\n",
    "# Equilibrage des données pour avoir le même nombre d'attrition oui et attrition non\n",
    "indices_0 = np.where(data_prepared_df[\"Attrition\"] == 0)[0]  # Indices où y=0\n",
    "indices_1 = np.where(data_prepared_df[\"Attrition\"] == 1)[0]  # Indices où y=1\n",
    "indices_0_under = np.random.choice(indices_0, size=len(indices_1), replace=False)\n",
    "indices_balanced = np.concatenate([indices_0_under, indices_1])\n",
    "\n",
    "indices_balanced_pd = pd.DataFrame(data_prepared_df.loc[indices_balanced], columns=columns)\n",
    "\n",
    "indices_balanced_pd[\"Attrition\"].value_counts()\n",
    "\n",
    "X = indices_balanced_pd.drop(\"Attrition\", axis=1)\n",
    "y = indices_balanced_pd[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des models\n",
    "models = {\n",
    "  #\"Perceptron\": Perceptron(random_state=42),\n",
    "  #\"LogisticRegression\": LogisticRegression(random_state=42),\n",
    "  \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "  \"RandomForest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# division du dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrinement des modèles\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions[name] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer les courbes de Précision-Rappel\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "        plt.plot(recall, precision, label=f\"{name}\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Tracer les courbes ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des scores de précision, rappel et F1\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "score_df = pd.DataFrame({\n",
    "    'Model': models.keys(),\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1 Score': f1_scores\n",
    "})\n",
    "\n",
    "score_df.plot(kind='box', figsize=(10, 6))\n",
    "plt.title(\"Comparison of Precision, Recall and F1 Scores\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des scores\n",
    "scores = []\n",
    "\n",
    "for name, y_pred in predictions.items():\n",
    "    y_proba = models[name].predict_proba(X_test)[:, 1] if hasattr(models[name], \"predict_proba\") else None\n",
    "    auc_score = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    scores.append({\n",
    "        'Model': name,\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'AUC': auc_score\n",
    "    })\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des temps d'entrainement et de prédiction\n",
    "trainingTimes = []\n",
    "predictionTimes = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    trainingTimes.append(time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.predict(X_test)\n",
    "    predictionTimes.append(time.time() - start_time)\n",
    "    \n",
    "time_df = pd.DataFrame({\n",
    "    'Model': models.keys(),\n",
    "    'Training Time (s)': trainingTimes,\n",
    "    'Prediction Time (s)': predictionTimes\n",
    "})\n",
    "print(time_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recherche d'hyperparamètre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dictionnaire pour les résultats de recherche\n",
    "results = {}\n",
    "\n",
    "# définition des hyperparamètres par model\n",
    "modelsWithHyperParams = {\n",
    "  \"DecisionTree\": (DecisionTreeClassifier(random_state=42), {\"criterion\": [\"gini\", \"entropy\", \"log_loss\"], \"splitter\": [\"best\", \"random\"], \"max_depth\": [None, 3, 5, 10], \"min_samples_leaf\": [1, 3, 5], \"min_samples_split\": [2, 3, 4]}),\n",
    "  \"RandomForest\": (RandomForestClassifier(random_state=42), {\"n_estimators\": [100, 200, 300], \"criterion\": [\"gini\", \"entropy\", \"log_loss\"], \"max_depth\": [None, 3, 5, 10], \"min_samples_leaf\": [1, 3, 5], \"min_samples_split\": [2, 3, 4]})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# recherche des meilleurs hyperparamètres\n",
    "for name, (model, params) in modelsWithHyperParams.items():\n",
    "  start_time = time.time()\n",
    "  grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "  grid_search.fit(X, y)\n",
    "  print(f\"{name} ({(time.time() - start_time):.2f}s): {grid_search.best_params_}\")\n",
    "  results[name] = (grid_search.best_params_, time.time() - start_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des résultats\n",
    "for name, (params, time) in results.items():\n",
    "  print(f\"{name} ({time:.2f}s): {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline avec paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des models avec les hyper paramètres trouvé plus haut\n",
    "modelsWithParams = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42, criterion='gini', max_depth=None, splitter='random', min_samples_leaf=1, min_samples_split=2),\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42, criterion='gini', max_depth=None, n_estimators=100, min_samples_leaf=1, min_samples_split=2 ) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des temps d'entrainement et de prédiction avec les hyperparamètres\n",
    "trainingTimesWithParams = []\n",
    "predictionTimesWithParams = []\n",
    "\n",
    "predictionsWithParams = {}\n",
    "for name, model in modelsWithParams.items():\n",
    "    startTime = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    trainingTimesWithParams.append(time.time() - startTime)\n",
    "    startTime = time.time()\n",
    "    predictionsWithParams[name] = model.predict(X_test)\n",
    "    predictionTimesWithParams.append(time.time() - startTime)\n",
    "\n",
    "time_df_params = pd.DataFrame({\n",
    "    'Model': modelsWithParams.keys(),\n",
    "    'Training Time (s)': trainingTimesWithParams,\n",
    "    'Prediction Time (s)': predictionTimesWithParams\n",
    "})\n",
    "print(time_df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparaison avec les temps sans hyperparamètres\n",
    "for index in range(len(models)):\n",
    "    print(f\"différence de temps d'entrainement pour le {time_df[\"Model\"][index]} : {(time_df[\"Training Time (s)\"][index] - time_df_params[\"Training Time (s)\"][index]):.5f}s c'est à dire une amélioration de {100 - (100 * time_df_params[\"Training Time (s)\"][index] / time_df[\"Training Time (s)\"][index]):.0f}%\")\n",
    "    print(f\"différence de temps de prédiction pour le {time_df[\"Model\"][index]} : {(time_df[\"Prediction Time (s)\"][index] - time_df_params[\"Prediction Time (s)\"][index]):.5f}s c'est à dire une amélioration de {100 - (100 * time_df_params[\"Prediction Time (s)\"][index] / time_df[\"Prediction Time (s)\"][index]):.0f}%\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsWithParams = {}\n",
    "\n",
    "for name, model in modelsWithParams.items():\n",
    "    predicted = cross_val_predict(model, X, y, cv=5) # KFold(n_splits=2)\n",
    "    score = cross_validate(model, X, y, cv=5, return_train_score=True, return_estimator=True)\n",
    "    predictionsWithParams[name] = (predicted, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (predicted, score) in predictionsWithParams.items():\n",
    "    print(name)\n",
    "    print(f\"accuracy score : {accuracy_score(y, predicted)}\")\n",
    "    print(f\"precision score : {precision_score(y, predicted)}\")\n",
    "    print(f\"f1 score : {f1_score(y, predicted)}\")\n",
    "    print(f\"recall score : {recall_score(y, predicted)}\")\n",
    "    print(f\"---\")\n",
    "    print(f\"train score : {score['train_score']}\")\n",
    "    print(f\"train score mean : {score['train_score'].mean()}\")\n",
    "    print(f\"test score : {score['test_score']}\")\n",
    "    print(f\"test score mean : {score['test_score'].mean()}\")\n",
    "    print(f\"---\")\n",
    "    plt.figure()\n",
    "    sns.heatmap(confusion_matrix(y, predicted), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Matrice de Confusion pour {name}\")\n",
    "    plt.xlabel(\"Classe Prédites\")\n",
    "    plt.ylabel(\"Classe Réelles\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (predicted, score) in predictionsWithParams.items():\n",
    "    if name == \"DecisionTree\":\n",
    "        fig = plt.figure(figsize=(25,10), dpi=800, linewidth=0.1)\n",
    "        plot_tree(score[\"estimator\"][0], feature_names=X.columns, class_names=['Attrition', \"Pas Attrition\"], filled=True)\n",
    "        plt.show()\n",
    "    elif name == \"RandomForest\":\n",
    "        fig, axes = plt.subplots(nrows = 1,ncols = 5,figsize = (130,10), dpi=800)\n",
    "        fig.tight_layout()\n",
    "        for index in range(0, 5):\n",
    "            plot_tree(score[\"estimator\"][0][index], feature_names = X.columns, class_names=['Attrition', \"Pas Attrition\"], filled = True, ax = axes[index]);\n",
    "            axes[index].set_title('Estimator: ' + str(index), fontsize = 11)\n",
    "        plt.show()\n",
    "    else :\n",
    "        print(\"No tree to plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
