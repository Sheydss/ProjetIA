{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron : \n",
    "    skipped\n",
    "\n",
    "### Logistic regression : \n",
    "    skipped\n",
    "\n",
    "### Naive Bayes : \n",
    "    skipped\n",
    "\n",
    "---\n",
    "\n",
    "### SVM (useless) : \n",
    "- C : https://scikit-learn.org/dev/auto_examples/svm/plot_svm_scale_c.html#sphx-glr-auto-examples-svm-plot-svm-scale-c-py\n",
    "(besoin de plus de détail)\n",
    "- kernel : https://scikit-learn.org/dev/auto_examples/svm/plot_svm_kernels.html#sphx-glr-auto-examples-svm-plot-svm-kernels-py\n",
    "- degree : degrée de la fonction polynomial quand kernel est à 'poly' cf kernel\n",
    "- gamma : coefficient utilisé par les kernel 'rbf', 'poly' et 'sigmoid' (besoin de plus de détail) cf kernel\n",
    "- coef0 : term utilisé dans la fonction du kernel. Significatif seulement pour 'rbf' et 'sigmoid' (besoin de plus de détail) cf kernel\n",
    "- shrinking : https://scikit-learn.org/dev/modules/svm.html#shrinking-svm\n",
    "(besoin de plus de détail)\n",
    "- probability : https://scikit-learn.org/dev/modules/svm.html#scores-probabilities\n",
    "- tol : tolérance pour le critèrion d'arrêt\n",
    "- cache_size : taille en MB du cache du kernel\n",
    "- class_weight : liste des poids des classes. On peut mettre soit un dictionnaire, soit utiliser le mode balanced (utilise la valeur de y pour ajuster automatiquement les poids de manière inversement proportionnelle aux fréquences des classes) ou rien dans ce cas tous les poids font 1.\n",
    "- decision_function_shape : soit utilise la fonction de décision one-vs-rest ('ovr') (comme tous les autre classifier) ou utilise la fonction one-vs-one ('ovo'). \n",
    "- break_ties: si utiliser, que l'on utilise la fonction 'ovr' et que l'on as plus que 2 classes, alors les ex-aqueo seront choisis par rapport à la confiance de la fonction de décision sinon on prend la première retourné.\n",
    "\n",
    "### KNN (useless) : \n",
    "- n_neighbors : nombre de voisins à utiliser\n",
    "- weights : fonction à utiliser pour les prédictions. \n",
    "    - 'uniform' : tous les points ont les mêmes poids \n",
    "    - 'distance' : les poids sont égals à l'inverse de la distance\n",
    "    - fonction personnalisé\n",
    "- algorithme : algorithme utilisé pour calculé le voisin le plus proche\n",
    "    - 'ball_tree' : https://en.wikipedia.org/wiki/Ball_tree\n",
    "    - 'kd_tree' : https://en.wikipedia.org/wiki/K-d_tree\n",
    "    - 'brute' : https://fr.wikipedia.org/wiki/Recherche_exhaustive\n",
    "    - 'auto' : essaye de décidé l'algorithme le plus approprié en fonction des valeurs dans le fit\n",
    "- Leaf_size : paramètre passé au algorithme ball_tree et kd_tree.\n",
    "- p : paramètre pour la Minkowski metric. Quand p = 1 c'est équivalent à la distance manhattan, quand p = 2 c'est équivalent à la distance euclidienne autrement c'est la Minkowski metric\n",
    "- metric : \n",
    "    - si c'est 'minkowski' alors on utilise p pour calculer soit la distance manhattan, soit la distance euclidienne. https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "    - si 'precomputed' alors la X est supposé être une matrice carré des distances \n",
    "    - sinon ça peut être une fonction qui prend deux array en une dimension\n",
    "- metric_params : arguments supplémentaire pour la fonction metric\n",
    "-njobs : le nombre de processus différent pour la recherche de voisins\n",
    "\n",
    "### Decision Tree : \n",
    "- criterion : Fonctions pour mesurer la qualité d'une division, 'gini', 'entropy' ou 'log_loss'. https://scikit-learn.org/1.5/modules/tree.html#tree-mathematical-formulation\n",
    "- splitter : Stratégies pour choisir la division. Soit aléatoire soit meilleur.\n",
    "- max_depth : La profondeur maximal de l'arbre. \n",
    "- min_samples_split : Le nombre minimum d'échantillons requis pour diviser un noeud interne\n",
    "- min_samples_leaf : Le nombre minimum d'échantillons requis pour être une feuille\n",
    "- min_weight_fraction_leaf : La fraction pondérée minimale de la somme totale des poids  requise pour se trouver à une feuille.\n",
    "- max_features : le nombre de caractérisitques à considérer lorsque l'on cherche le meilleur split. \n",
    "    - int => max_features=max_features\n",
    "    - float => max_features=max(1, int(max_features * n_features_in_))\n",
    "    - sqrt => max_features=sqrt(n_features)\n",
    "    - log2 => max_features=log2(n_features)\n",
    "    - none => max_features=n_features)\n",
    "- max_leaf_nodes : Faire croître un arbre avec max_leaf_nodes de la meilleure façon possible. Les meilleurs nœuds sont définis comme une réduction relative de l'impureté.\n",
    "- min_impurity_decrease : Un noeud est divisé si la séparation induit une diminution de l'impureté plus grand ou égal à cette valeur\n",
    "L'équation pour calculer la baisse d'impureté est : N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity)\n",
    "- class_weight : Liste des poids des classes. On peut mettre soit un dictionnaire, soit utiliser le mode balanced (utilise la valeur de y pour ajuster automatiquement les poids de manière inversement proportionnelle aux fréquences des classes) ou rien dans ce cas tous les poids font 1.\n",
    "- ccp_alpha : Paramètre de complexité utilisé pour l'élagage coût minimal-complexité. Le sous-arbre dont la complexité des coûts est la plus grande et qui est plus petit que ccp_alpha sera choisi.\n",
    "- monotonic_cst : Indique la contrainte de monotonicité à appliquer à chaque caractéristique.\n",
    "    - 1 : augmentation monotone\n",
    "    - 0 : pas de contrainte\n",
    "    - -1 : diminution monotone  \n",
    "If monotonic_cst is None, no constraints are applied.\n",
    "---\n",
    "\n",
    "### Random Forest : \n",
    "- n_estimator : Nombre d'arbre dans la forêt\n",
    "- criterion : Fonctions pour mesurer la qualité d'une division, 'gini', 'entropy' ou 'log_loss'. https://scikit-learn.org/1.5/modules/tree.html#tree-mathematical-formulation\n",
    "- max_depth : La profondeur maximal de l'arbre. \n",
    "- min_samples_split : Le nombre minimum d'échantillons requis pour diviser un noeud interne\n",
    "- min_samples_leaf : Le nombre minimum d'échantillons requis pour être une feuille\n",
    "- min_weight_fraction_leaf : La fraction pondérée minimale de la somme totale des poids  requise pour se trouver à une feuille.\n",
    "- max_features : le nombre de caractérisitques à considérer lorsque l'on cherche le meilleur split. \n",
    "    - int => max_features=max_features\n",
    "    - float => max_features=max(1, int(max_features * n_features_in_))\n",
    "    - sqrt => max_features=sqrt(n_features)\n",
    "    - log2 => max_features=log2(n_features)\n",
    "    - none => max_features=n_features)\n",
    "- max_leaf_nodes : Faire croître un arbre avec max_leaf_nodes de la meilleure façon possible. Les meilleurs nœuds sont définis comme une réduction relative de l'impureté.\n",
    "- min_impurity_decrease : Un noeaud est divisé si la séparation induit une diminution de l'impureté plus grand ou égal à cette valeur\n",
    "- bootstrap : Le bootstrapping est une technique de rééchantillonnage statistique qui implique un échantillonnage aléatoire d'un ensemble de données avec remplacement. Si false tous le dataset est utilisé pour chaque arbre\n",
    "- oob_score : Indique s'il faut utiliser les échantillons out-of-bag pour estimer le score de généralisation. Par défaut, c'est le score de précision qui est utilisé.\n",
    "- Le nombre de jobs à utilisé en parallèles. fit, predict, decision_path et apply sont tous parallélisé sur les arbres.\n",
    "- warm_start : Permet de réutiliser la solution de l'appel précédent pour ajuster et ajouter d'autres estimateurs à l'ensemble, sinon, ajuster une toute nouvelle forêt.\n",
    "- class_weight : Liste des poids des classes. On peut mettre soit un dictionnaire, soit utiliser le mode balanced (utilise la valeur de y pour ajuster automatiquement les poids de manière inversement proportionnelle aux fréquences des classes) ou rien dans ce cas tous les poids font 1.\n",
    "- ccp_alpha : Paramètre de complexité utilisé pour l'élagage coût minimal-complexité. Le sous-arbre dont la complexité des coûts est la plus grande et qui est plus petit que ccp_alpha sera choisi.\n",
    "- max_samples : Nombre maximal d'échantillons à prendre de X pour entrainer chaque estimateur.\n",
    "- monotonic_cst : Indique la contrainte de monotonicité à appliquer à chaque caractéristique.\n",
    "    - 1 : augmentation monotone\n",
    "    - 0 : pas de contrainte\n",
    "    - -1 : diminution monotone  \n",
    "If monotonic_cst is None, no constraints are applied.\n",
    "\n",
    "Note :\n",
    "- j'ai skip les random_states et verbose car ils ne sont pas pertinent dans cette \"analyse\" "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
